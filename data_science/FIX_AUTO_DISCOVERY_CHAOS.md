# ğŸš¨ CRITICAL BUG: Auto-Discovery Processing Multiple Files

## ğŸ” Problem Found

When user uploads ONE file, the system is:
1. âŒ Discovering ALL old CSV files on disk
2. âŒ Processing each discovered file
3. âŒ Creating workspaces for each file
4. âŒ Calling old `dataset_workspace_manager` system (creates hash-based folders)

## ğŸ“ Root Causes

### Cause #1: `rehydrate_session_state()` - Lines 764-782

**Location:** `artifact_manager.py`

```python
def rehydrate_session_state(state: Dict[str, Any]) -> None:
    # ...
    
    # If missing or not on disk, discover freshest on disk under UPLOAD_ROOT
    def _latest(patterns):
        from glob import glob
        candidates: list[str] = []
        # Search MULTIPLE locations including ALL subdirectories
        likely_roots = [
            _UPLOAD_ROOT,
            os.path.join(_UPLOAD_ROOT, ".uploaded"),
            os.path.join(_UPLOAD_ROOT, "uploads")
        ]
        for root in dict.fromkeys([p for p in likely_roots if p]):
            for pat in patterns:
                # âŒ RECURSIVE SEARCH finds ALL old files!
                candidates += glob(os.path.join(root, "**", pat), recursive=True)
        # ...
        return max(candidates, key=os.path.getmtime)  # â† Picks latest from ALL files
    
    if not csv_path or not os.path.exists(csv_path):
        candidate = _latest(["*.csv", "*.parquet"])  # â† Searches EVERYTHING!
        if candidate:
            state["default_csv_path"] = candidate  # â† Binds random old file!
```

**Problem:** Searches recursively through ALL workspace folders and binds random old files!

---

### Cause #2: `ensure_dataset_binding()` - Lines 316-332

**Location:** `agent.py`

```python
def ensure_dataset_binding(tool_context):
    # ...
    
    # Try workspace/uploads first
    ws_paths = state.get("workspace_paths", {})
    uploads_dir = ws_paths.get("uploads") or upload_root
    candidates = []
    
    for ext in ("*.csv",):
        # âŒ RECURSIVE search finds ALL old files!
        candidates += glob.glob(os.path.join(uploads_dir, "**", ext), recursive=True)
    
    if not candidates:
        # Try global upload_root fallback
        for ext in ("*.csv",):
            # âŒ ANOTHER recursive search!
            candidates += glob.glob(os.path.join(upload_root, "**", ext), recursive=True)
    
    # Pick the most recent file
    latest_file = max(candidates, key=os.path.getmtime)  # â† Random old file!
    state["default_csv_path"] = latest_file
```

**Problem:** Searches recursively and binds ANY old CSV it finds!

---

### Cause #3: `workspace_tools.py` Still Imports Old System

**Location:** `workspace_tools.py` lines 39, 108, 178, 247

```python
from .dataset_workspace_manager import (
    create_workspace_structure,  # â† OLD system with hash-based folders!
    ...
)
```

**Problem:** Old workspace manager creates folders like:
- `student_portuguese_clean_6af3b204` (hash suffix)
- `student_portuguese_clean_utf8_e117a84f` (hash suffix)

---

## ğŸ“Š Evidence from User's Directory

```
11/01/2025  05:20 PM    ads50                                    â† NEW system âœ…
11/01/2025  05:22 PM    student_portuguese_clean_6af3b204        â† OLD system âŒ
11/01/2025  05:23 PM    student_portuguese_clean_utf8_e117a84f   â† OLD system âŒ
11/01/2025  05:23 PM    default                                  â† Fallback âŒ
11/01/2025  05:23 PM    uploaded                                 â† Fallback âŒ
11/01/2025  05:22 PM    _global                                  â† Fallback âŒ
```

**Timestamps show:**
- 5:20 PM: First workspace created (ads50)
- 5:22 PM: Old system triggered (hash-based folders)
- 5:23 PM: Fallback logic triggered (default, uploaded)

**This means:** Within 3 minutes, THREE different workspace creation systems ran!

---

## ğŸ¯ The Flow (What's Actually Happening)

```
User uploads: tips.csv
    â†“
1. NEW system (artifact_manager.ensure_workspace)
   â†’ Creates: tips/20251101_172000/ âœ…
    â†“
2. rehydrate_session_state() is called
   â†’ Searches ALL directories recursively
   â†’ Finds: old_file1.csv, old_file2.csv, old_file3.csv
   â†’ Binds: old_file3.csv (most recent mtime)
    â†“
3. ensure_dataset_binding() is called
   â†’ Searches AGAIN recursively
   â†’ Finds MORE old files
   â†’ Processes each one
    â†“
4. workspace_tools calls OLD dataset_workspace_manager
   â†’ Creates: student_portuguese_clean_6af3b204/ âŒ (hash-based!)
    â†“
5. Fallback logic triggers
   â†’ Creates: default/, _global/, uploaded/ âŒ
    â†“
RESULT: 6 workspace folders for ONE upload! âŒâŒâŒ
```

---

## âœ… Solution

### Fix #1: Disable Recursive Search in `rehydrate_session_state`

**Location:** `artifact_manager.py` line 776

```python
# BEFORE (BROKEN):
candidates += glob(os.path.join(root, "**", pat), recursive=True)  # â† Searches EVERYWHERE!

# AFTER (FIXED):
candidates += glob(os.path.join(root, pat), recursive=False)  # â† Only top level!
```

### Fix #2: Disable Recursive Search in `ensure_dataset_binding`

**Location:** `agent.py` lines 317, 322

```python
# BEFORE (BROKEN):
candidates += glob.glob(os.path.join(uploads_dir, "**", ext), recursive=True)

# AFTER (FIXED):
candidates += glob.glob(os.path.join(uploads_dir, ext), recursive=False)
```

### Fix #3: Only Bind CURRENT Upload, Not Old Files

**Add guard to rehydrate_session_state:**

```python
def rehydrate_session_state(state: Dict[str, Any]) -> None:
    # ...
    
    # âœ… NEW: Only search if NO file is already bound in this session
    if state.get("default_csv_path") and os.path.exists(state["default_csv_path"]):
        logger.info(f"[REHYDRATE] CSV already bound: {state['default_csv_path']}, skipping search")
        return  # â† Don't search for other files!
    
    # Only search if truly needed (no file in current session)
    if not csv_path or not os.path.exists(csv_path):
        # ... existing search logic
```

### Fix #4: Prevent workspace_tools from Calling Old System

**Option A:** Remove `workspace_tools` from agent registration  
**Option B:** Fix `workspace_tools.py` to use NEW `artifact_manager.ensure_workspace` instead

---

## ğŸ”§ User's Suggestion: "Use session ID per dataset"

**Already implemented!** But being bypassed by auto-discovery.

Current logic:
```python
# Line 311-314: artifact_manager.py
run_id = callback_state.get("workspace_run_id")
if not run_id:
    run_id = time.strftime("%Y%m%d_%H%M%S")  # â† Session-based timestamp
    callback_state["workspace_run_id"] = run_id  # â† Persisted!
```

**This works perfectly!** The problem is that auto-discovery is:
1. Finding old files
2. Processing them
3. Each creates a NEW session with NEW run_id
4. Result: Multiple folders

---

## ğŸ“‹ Implementation Plan

### Priority 1: Stop Processing Old Files (CRITICAL)

1. âœ… Modify `rehydrate_session_state`: Remove `recursive=True`
2. âœ… Modify `ensure_dataset_binding`: Remove `recursive=True`
3. âœ… Add guard: Only search if NO file bound in current session

### Priority 2: Fix workspace_tools

1. âœ… Remove `workspace_tools` from agent tool registration
2. OR: Modify `workspace_tools.py` to use `artifact_manager.ensure_workspace`

### Priority 3: Cleanup

1. âœ… Delete old hash-based folders
2. âœ… Delete fallback folders (default, _global)

---

## ğŸ¯ Expected Result After Fix

```
User uploads: tips.csv (ONE file)
    â†“
1. NEW system creates: tips/20251101_172000/ âœ…
    â†“
2. rehydrate_session_state checks: File already bound? YES
   â†’ Skips search âœ…
    â†“
3. ensure_dataset_binding checks: File already bound? YES
   â†’ Skips search âœ…
    â†“
RESULT: ONE workspace folder for ONE upload! âœ…âœ…âœ…
```

---

**Status:** âš ï¸ CRITICAL BUG FOUND  
**Impact:** Creates 3-6 workspace folders per upload  
**Confidence:** 95% (Root cause identified with evidence)  
**Next Step:** Apply fixes to `artifact_manager.py` and `agent.py`

